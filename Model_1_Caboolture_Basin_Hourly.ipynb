{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Flood Prediction Model 1 - Hourly Discharge at Caboolture River\n",
    "\n",
    "The Objective of this notebook is to create a flood prediction model for the Caboolture River at Upper Caboolture. We will use hourly rainfall data and an hourly discharge as the target time series and a single hourly rainfall time series as input variable. This model will be used for comparision with [RORB](https://www.monash.edu/engineering/departments/civil/research/themes/water/rorb)â€™s predictions.\n",
    "\n",
    "## Benchmark Model\n",
    "\n",
    "[RORB](https://www.monash.edu/engineering/departments/civil/research/themes/water/rorb) model is generally employed for calculating design flood discharges. It uses many assumptions and is manually calibrated to one flooding event. This will be used as benchmark model for comparison purpose.\n",
    "\n",
    "## Data Set\n",
    "The hydrological data available at [Queensland Water Monitoring Information Portal](https://water-monitoring.information.qld.gov.au/) will be used. Hourly as well as daily water flow data is available at various stations. Rainfall data is available in some of the stations. For the Caboolture River, only single discharge and rainfal station is available. \n",
    "\n",
    "\n",
    "\n",
    "### Work Workflow\n",
    "\n",
    "* Preprocessing and exploring the data\n",
    "* Creating training and test sets of time series\n",
    "* Formatting data as JSON files and uploading to S3\n",
    "* Instantiating and training a DeepAR estimator\n",
    "* Deploying a model and creating a predictor\n",
    "* Comparing the Predictor with RORB's performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing and exploring the data\n",
    "\n",
    "The raw data for Caboolture River is available at raw_data/Caboolture folder. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "from utility import unzip_ts_data, read_ts_data, series_to_json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./raw_data/Caboolture/142001A_20191103_rain\n",
      "unzipped 142001A_20191103_rain.zip!\n",
      "./raw_data/Caboolture/142001A_20191103_flow\n",
      "unzipped 142001A_20191103_flow.zip!\n",
      "./raw_data/Maroochy/141004B_20191110_rain\n",
      "unzipped 141004B_20191110_rain.zip!\n",
      "./raw_data/Brisbane/143306B_20191112\n",
      "unzipped 143306B_20191112.zip!\n"
     ]
    }
   ],
   "source": [
    "# Read target (flow) and feature (rain) data\n",
    "\n",
    "target_name = '142001A_20191103_flow' # folder name containing target time series\n",
    "\n",
    "# Unzip the data and get list of csv paths\n",
    "cabooltre_csv_paths = unzip_ts_data('./raw_data/Caboolture')\n",
    "maroochy_csv_paths = unzip_ts_data('./raw_data/Maroochy')\n",
    "brisbane_csv_paths = unzip_ts_data('./raw_data/Brisbane')\n",
    "\n",
    "# Select the target csv\n",
    "target_csv_paths = []\n",
    "other_csv_paths = []\n",
    "for csv_path in cabooltre_csv_paths:\n",
    "    if target_name in csv_path:\n",
    "        target_csv_paths.append(csv_path)\n",
    "    else:\n",
    "        other_csv_paths.append(csv_path)\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./raw_data/Caboolture/142001A_20191103_rain/142001A.csv',\n",
       " './raw_data/Caboolture/142001A_20191103_flow/142001A.csv']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cabooltre_csv_paths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ec2-user/anaconda3/envs/amazonei_mxnet_p36/lib/python3.6/site-packages/pandas/core/frame.py:4025: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: http://pandas.pydata.org/pandas-docs/stable/indexing.html#indexing-view-versus-copy\n",
      "  return super(DataFrame, self).rename(**kwargs)\n"
     ]
    }
   ],
   "source": [
    "# Read the extracted csv data\n",
    "\n",
    "target_ts_data = read_ts_data(target_csv_paths[0], value_cols = [\"Mean\"], prefix = \"CabFlow\")\n",
    "\n",
    "rain_ts_data = read_ts_data(other_csv_paths[0], value_cols = [\"Total\"], prefix = \"CabRain\")\n",
    "\n",
    "rain_maroochy_ts_data = read_ts_data(maroochy_csv_paths[0], value_cols = [\"Total\"], prefix = \"MarRain\")\n",
    "rain_brisbane_ts_data = read_ts_data(brisbane_csv_paths[0], value_cols = [\"Total\"], prefix = \"BrisRain\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "main_ts_data = target_ts_data.join(rain_ts_data)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Values\n",
    "\n",
    "First we remove the missing values from the both ends of time. After doing so, we find that there are no missing values for the flow data while there are some missing data for the rainfall data. We replace the missing rainfall data with the rainfall from the nearest station in Maroochy basin."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CabFlowMean</th>\n",
       "      <th>CabRainTotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>248640.000000</td>\n",
       "      <td>244332.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.939243</td>\n",
       "      <td>0.128908</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.070325</td>\n",
       "      <td>1.135370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>567.881600</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CabFlowMean   CabRainTotal\n",
       "count  248640.000000  244332.000000\n",
       "mean        0.939243       0.128908\n",
       "std         8.070325       1.135370\n",
       "min         0.000000       0.000000\n",
       "25%         0.017500       0.000000\n",
       "50%         0.074000       0.000000\n",
       "75%         0.252000       0.000000\n",
       "max       567.881600      87.000000"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def first_last_non_null(all_ts_data):\n",
    "    \"\"\"\n",
    "    Returns the index for first and last non-null rows on data\n",
    "    \"\"\"\n",
    "    selec_non_null = np.apply_along_axis(np.sum, 1, ~pd.isnull(all_ts_data).values) >= 2\n",
    "    # Find the first and last non-null index for all data\n",
    "    first_index = all_ts_data.index[selec_non_null][0]\n",
    "    last_index = all_ts_data.index[selec_non_null][-1]\n",
    "\n",
    "    return first_index, last_index\n",
    "\n",
    "# Select for this period\n",
    "\n",
    "first_index, last_index = first_last_non_null(main_ts_data)\n",
    "\n",
    "selec_all_ts_data = main_ts_data.iloc[(main_ts_data.index >= first_index) & (main_ts_data.index <= last_index),]\n",
    "\n",
    "selec_all_ts_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace the missing rainfall with Maroochy rain\n",
    "\n",
    "selec2_all_ts_data = selec_all_ts_data.join(rain_maroochy_ts_data)\n",
    "\n",
    "selec2_all_ts_data.iloc[pd.isnull(selec2_all_ts_data.iloc[:,1]).values,1] = selec2_all_ts_data.iloc[pd.isnull(selec2_all_ts_data.iloc[:,1]).values,2]\n",
    "\n",
    "selec2_all_ts_data.drop(\"MarRainTotal\", 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CabFlowMean</th>\n",
       "      <th>CabRainTotal</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>248640.000000</td>\n",
       "      <td>248640.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.939243</td>\n",
       "      <td>0.129345</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.070325</td>\n",
       "      <td>1.138789</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.017500</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.252000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>567.881600</td>\n",
       "      <td>87.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         CabFlowMean   CabRainTotal\n",
       "count  248640.000000  248640.000000\n",
       "mean        0.939243       0.129345\n",
       "std         8.070325       1.138789\n",
       "min         0.000000       0.000000\n",
       "25%         0.017500       0.000000\n",
       "50%         0.074000       0.000000\n",
       "75%         0.252000       0.000000\n",
       "max       567.881600      87.000000"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "selec2_all_ts_data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1440x432 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Plot filter\n",
    "\n",
    "# display the data \n",
    "plt.figure(figsize=(20,6))\n",
    "# all data points\n",
    "selec2_all_ts_data.plot(title='Global active power', color=['blue','red']) \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spliting to Training and Test Series\n",
    "Now we split the data into training and test series. Data up to 2016 will be used for training and then the rest for testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_all_ts_data = selec2_all_ts_data.iloc[selec2_all_ts_data.index < pd.Timestamp('2016-01-01 00:00:00'),:]\n",
    "test_all_ts_data = selec2_all_ts_data.iloc[selec2_all_ts_data.index >= pd.Timestamp('2016-01-01 00:00:00'),:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Convert to JSON\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CabFlowMean</th>\n",
       "      <th>CabRainTotal</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>DateTime</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2016-01-01 00:00:00</th>\n",
       "      <td>0.0285</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 01:00:00</th>\n",
       "      <td>0.0280</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 02:00:00</th>\n",
       "      <td>0.0262</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 03:00:00</th>\n",
       "      <td>0.0255</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2016-01-01 04:00:00</th>\n",
       "      <td>0.0260</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     CabFlowMean  CabRainTotal\n",
       "DateTime                                      \n",
       "2016-01-01 00:00:00       0.0285           0.0\n",
       "2016-01-01 01:00:00       0.0280           0.0\n",
       "2016-01-01 02:00:00       0.0262           0.0\n",
       "2016-01-01 03:00:00       0.0255           0.0\n",
       "2016-01-01 04:00:00       0.0260           0.0"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_ts_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.0285, 0.028 , 0.0262, ..., 0.009 , 0.009 , 0.009 ],\n",
       "       [0.0285, 0.028 , 0.0262, ..., 0.009 , 0.009 , 0.009 ]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_col = \"CabFlowMean\"\n",
    "test_all_ts_data[[target_col,target_col]].values.transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['CabFlowMean', 'CabRainTotal'], dtype=object)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_ts_data.columns.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_col = \"CabFlowMean\"\n",
    "train_json = series_to_json(train_all_ts_data, target_col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save Data Locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import json for formatting data\n",
    "import json\n",
    "import os # and os for saving\n",
    "\n",
    "def write_json_dataset(time_series, target_col, filename): \n",
    "    with open(filename, 'wb') as f:\n",
    "        json_line = json.dumps(series_to_json(time_series, target_col)) + '\\n'\n",
    "        json_line = json_line.encode('utf-8')\n",
    "        f.write(json_line)\n",
    "    print(filename + ' saved.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save this data to a local directory\n",
    "data_dir = 'json_model1_data'\n",
    "\n",
    "# make data dir, if it does not exist\n",
    "if not os.path.exists(data_dir):\n",
    "    os.makedirs(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "json_model1_data/train.json saved.\n",
      "json_model1_data/test.json saved.\n"
     ]
    }
   ],
   "source": [
    "# directories to save train/test data\n",
    "train_path = os.path.join(data_dir, 'train.json')\n",
    "test_path = os.path.join(data_dir, 'test.json')\n",
    "\n",
    "# write train/test JSON files\n",
    "write_json_dataset(train_all_ts_data, target_col, train_path)        \n",
    "write_json_dataset(train_all_ts_data, target_col, test_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Uploading data to S3\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import sagemaker\n",
    "from sagemaker import get_execution_role"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# session, role, bucket\n",
    "sagemaker_session = sagemaker.Session()\n",
    "role = get_execution_role()\n",
    "\n",
    "bucket = sagemaker_session.default_bucket()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload training data to a location in S3, and save that location to train_path\n",
    "Upload test data to a location in S3, and save that location to test_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [],
   "source": [
    "prefix='flood-prediction-model-1'\n",
    "\n",
    "# *unique* train/test prefixes\n",
    "train_prefix   = '{}/{}'.format(prefix, 'train')\n",
    "test_prefix    = '{}/{}'.format(prefix, 'test')\n",
    "\n",
    "# uploading data to S3, and saving locations\n",
    "train_s3_path  = sagemaker_session.upload_data(train_path, bucket=bucket, key_prefix=train_prefix)\n",
    "test_s3_path   = sagemaker_session.upload_data(test_path,  bucket=bucket, key_prefix=test_prefix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training data is stored in: s3://sagemaker-ap-southeast-2-990878777707/flood-prediction-model-1/train/train.json\n",
      "Test data is stored in: s3://sagemaker-ap-southeast-2-990878777707/flood-prediction-model-1/test/test.json\n"
     ]
    }
   ],
   "source": [
    "# check locations\n",
    "print('Training data is stored in: '+ train_s3_path)\n",
    "print('Test data is stored in: '+ test_s3_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Training a DeepAR Estimator\n",
    "\n",
    "Some estimators have specific, SageMaker constructors, but not all. Instead you can create a base `Estimator` and pass in the specific image (or container) that holds a specific model.\n",
    "\n",
    "Next, we configure the container image to be used for the region that we are running in."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.amazon.amazon_estimator import get_image_uri\n",
    "\n",
    "image_name = get_image_uri(boto3.Session().region_name, # get the region\n",
    "                           'forecasting-deepar') # specify image\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instantiate an Estimator \n",
    "\n",
    "You can now define the estimator that will launch the training job. A generic Estimator will be defined by the usual constructor arguments and an `image_name`. \n",
    "> You can take a look at the [estimator source code](https://github.com/aws/sagemaker-python-sdk/blob/master/src/sagemaker/estimator.py#L595) to view specifics.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.estimator import Estimator\n",
    "\n",
    "# dir to save model artifacts\n",
    "s3_output_path = \"s3://{}/{}/output\".format(bucket, prefix)\n",
    "\n",
    "# instantiate a DeepAR estimator\n",
    "estimator = Estimator(sagemaker_session=sagemaker_session,\n",
    "                      image_name=image_name,\n",
    "                      role=role,\n",
    "                      train_instance_count=1,\n",
    "                      train_instance_type='ml.c4.2xlarge',\n",
    "                      output_path=s3_output_path\n",
    "                      )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Hyperparameters\n",
    "\n",
    "Next, we need to define some DeepAR hyperparameters that define the model size and training behavior. Values for the epochs, frequency, prediction length, and context length are required.\n",
    "\n",
    "* **epochs**: The maximum number of times to pass over the data when training.\n",
    "* **time_freq**: The granularity of the time series in the dataset ('D' for daily).\n",
    "* **prediction_length**: A string; the number of time steps (based off the unit of frequency) that the model is trained to predict. \n",
    "* **context_length**: The number of time points that the model gets to see *before* making a prediction. \n",
    "\n",
    "### Context Length\n",
    "\n",
    "Typically, it is recommended that you start with a `context_length`=`prediction_length`. This is because a DeepAR model also receives \"lagged\" inputs from the target time series, which allow the model to capture long-term dependencies. For example, a daily time series can have yearly seasonality and DeepAR automatically includes a lag of one year. So, the context length can be shorter than a year, and the model will still be able to capture this seasonality. \n",
    "\n",
    "The lag values that the model picks depend on the frequency of the time series. For example, lag values for daily frequency are the previous week, 2 weeks, 3 weeks, 4 weeks, and year. You can read more about this in the [DeepAR \"how it works\" documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_how-it-works.html).\n",
    "\n",
    "### Optional Hyperparameters\n",
    "\n",
    "You can also configure optional hyperparameters to further tune your model. These include parameters like the number of layers in our RNN model, the number of cells per layer, the likelihood function, and the training options, such as batch size and learning rate. \n",
    "\n",
    "For an exhaustive list of all the different DeepAR hyperparameters you can refer to the DeepAR [hyperparameter documentation](https://docs.aws.amazon.com/sagemaker/latest/dg/deepar_hyperparameters.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freq='H' # Hourly\n",
    "context_length = 24\n",
    "prediction_length = 7\n",
    "\n",
    "hyperparameters = {\n",
    "    \"epochs\": \"50\",\n",
    "    \"time_freq\": freq,\n",
    "    \"prediction_length\": str(prediction_length),\n",
    "    \"context_length\": str(context_length),\n",
    "    \"num_cells\": \"50\",\n",
    "    \"num_layers\": \"2\",\n",
    "    \"mini_batch_size\": \"128\",\n",
    "    \"learning_rate\": \"0.001\",\n",
    "    \"early_stopping_patience\": \"10\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "# set the hyperparams\n",
    "estimator.set_hyperparameters(**hyperparameters)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training Job\n",
    "\n",
    "Now, we are ready to launch the training job! SageMaker will start an EC2 instance, download the data from S3, start training the model and save the trained model.\n",
    "\n",
    "If you provide the `test` data channel, as we do in this example, DeepAR will also calculate accuracy metrics for the trained model on this test data set. This is done by predicting the last `prediction_length` points of each time series in the test set and comparing this to the *actual* value of the time series. The computed error metrics will be included as part of the log output.\n",
    "\n",
    "The next cell may take a few minutes to complete, depending on data size, model complexity, and training options."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2019-11-21 07:41:20 Starting - Starting the training job...\n",
      "2019-11-21 07:41:25 Starting - Launching requested ML instances......\n",
      "2019-11-21 07:42:50 Starting - Preparing the instances for training......\n",
      "2019-11-21 07:43:36 Downloading - Downloading input data...\n",
      "2019-11-21 07:44:16 Training - Training image download completed. Training in progress..\u001b[31mArguments: train\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:18 INFO 140411496507200] Reading default configuration from /opt/amazon/lib/python2.7/site-packages/algorithm/resources/default-input.json: {u'num_dynamic_feat': u'auto', u'dropout_rate': u'0.10', u'mini_batch_size': u'128', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'_num_gpus': u'auto', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_cells': u'40', u'num_layers': u'2', u'embedding_dimension': u'10', u'_kvstore': u'auto', u'_num_kv_servers': u'auto', u'cardinality': u'auto', u'likelihood': u'student-t', u'early_stopping_patience': u''}\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:18 INFO 140411496507200] Reading provided configuration from /opt/ml/input/config/hyperparameters.json: {u'learning_rate': u'0.001', u'num_cells': u'50', u'prediction_length': u'7', u'epochs': u'50', u'time_freq': u'H', u'context_length': u'24', u'num_layers': u'2', u'mini_batch_size': u'128', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:18 INFO 140411496507200] Final configuration: {u'dropout_rate': u'0.10', u'test_quantiles': u'[0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9]', u'_tuning_objective_metric': u'', u'num_eval_samples': u'100', u'learning_rate': u'0.001', u'num_layers': u'2', u'epochs': u'50', u'embedding_dimension': u'10', u'num_cells': u'50', u'_num_kv_servers': u'auto', u'mini_batch_size': u'128', u'likelihood': u'student-t', u'num_dynamic_feat': u'auto', u'cardinality': u'auto', u'_num_gpus': u'auto', u'prediction_length': u'7', u'time_freq': u'H', u'context_length': u'24', u'_kvstore': u'auto', u'early_stopping_patience': u'10'}\u001b[0m\n",
      "\u001b[31mProcess 1 is a worker.\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:18 INFO 140411496507200] Detected entry point for worker worker\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:18 INFO 140411496507200] Using early stopping with patience 10\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] [cardinality=auto] `cat` field was NOT found in the file `/opt/ml/input/data/train/train.json` and will NOT be used for training.\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] [num_dynamic_feat=auto] `dynamic_feat` field was found in the file `/opt/ml/input/data/train/train.json` and will be used for training.\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] [num_dynamic_feat=auto] Inferred value of num_dynamic_feat=1 from dataset.\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] Training set statistics:\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] Real time series\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] number of time series: 1\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] number of observations: 221352\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] mean target length: 221352\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] min/mean/max target: 0.0/0.969651773533/567.881591797\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] mean abs(target): 0.969651773533\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] contains missing values: no\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] Small number of time series. Doing 10 number of passes over dataset per epoch.\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] Test set statistics:\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] Real time series\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] number of time series: 1\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] number of observations: 221352\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] mean target length: 221352\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] min/mean/max target: 0.0/0.969651773533/567.881591797\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] mean abs(target): 0.969651773533\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] contains missing values: no\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] nvidia-smi took: 0.0251779556274 secs to identify 0 gpus\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:19 INFO 140411496507200] Create Store: local\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 881.270170211792, \"sum\": 881.270170211792, \"min\": 881.270170211792}}, \"EndTime\": 1574322260.001016, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322259.118911}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:20 INFO 140411496507200] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"initialize.time\": {\"count\": 1, \"max\": 1243.852138519287, \"sum\": 1243.852138519287, \"min\": 1243.852138519287}}, \"EndTime\": 1574322260.362886, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322260.001119}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:29 INFO 140411496507200] Epoch[0] Batch[0] avg_epoch_loss=0.129919\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:29 INFO 140411496507200] #quality_metric: host=algo-1, epoch=0, batch=0 train loss <loss>=0.12991926074\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:31 INFO 140411496507200] Epoch[0] Batch[5] avg_epoch_loss=-0.579798\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:31 INFO 140411496507200] #quality_metric: host=algo-1, epoch=0, batch=5 train loss <loss>=-0.579797831674\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:31 INFO 140411496507200] Epoch[0] Batch [5]#011Speed: 523.28 samples/sec#011loss=-0.579798\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] Epoch[0] Batch[10] avg_epoch_loss=-0.983223\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] #quality_metric: host=algo-1, epoch=0, batch=10 train loss <loss>=-1.46733293533\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] Epoch[0] Batch [10]#011Speed: 428.23 samples/sec#011loss=-1.467333\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] processed a total of 1356 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"epochs\": {\"count\": 1, \"max\": 50, \"sum\": 50.0, \"min\": 50}, \"update.time\": {\"count\": 1, \"max\": 12313.535928726196, \"sum\": 12313.535928726196, \"min\": 12313.535928726196}}, \"EndTime\": 1574322272.67655, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322260.362945}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=110.121731423 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] #progress_metric: host=algo-1, completed 2 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] #quality_metric: host=algo-1, epoch=0, train loss <loss>=-0.983222878792\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:32 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_f0df98e2-7a1d-4a71-b142-e0da7bf154c5-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 16.482830047607422, \"sum\": 16.482830047607422, \"min\": 16.482830047607422}}, \"EndTime\": 1574322272.693616, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322272.676628}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:35 INFO 140411496507200] Epoch[1] Batch[0] avg_epoch_loss=-1.454976\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:35 INFO 140411496507200] #quality_metric: host=algo-1, epoch=1, batch=0 train loss <loss>=-1.45497643948\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:36 INFO 140411496507200] Epoch[1] Batch[5] avg_epoch_loss=-1.496254\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:36 INFO 140411496507200] #quality_metric: host=algo-1, epoch=1, batch=5 train loss <loss>=-1.49625404676\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:36 INFO 140411496507200] Epoch[1] Batch [5]#011Speed: 533.46 samples/sec#011loss=-1.496254\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] Epoch[1] Batch[10] avg_epoch_loss=-1.190634\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] #quality_metric: host=algo-1, epoch=1, batch=10 train loss <loss>=-0.823890471458\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] Epoch[1] Batch [10]#011Speed: 416.12 samples/sec#011loss=-0.823890\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5788.287162780762, \"sum\": 5788.287162780762, \"min\": 5788.287162780762}}, \"EndTime\": 1574322278.482021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322272.693677}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=222.85950546 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] #progress_metric: host=algo-1, completed 4 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] #quality_metric: host=algo-1, epoch=1, train loss <loss>=-1.1906342398\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:38 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_48185582-e5b8-4fa4-b916-3bfd008c968f-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 14.735937118530273, \"sum\": 14.735937118530273, \"min\": 14.735937118530273}}, \"EndTime\": 1574322278.497352, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322278.482099}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:41 INFO 140411496507200] Epoch[2] Batch[0] avg_epoch_loss=-1.843120\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:41 INFO 140411496507200] #quality_metric: host=algo-1, epoch=2, batch=0 train loss <loss>=-1.8431198597\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:42 INFO 140411496507200] Epoch[2] Batch[5] avg_epoch_loss=-1.698780\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:42 INFO 140411496507200] #quality_metric: host=algo-1, epoch=2, batch=5 train loss <loss>=-1.69877992074\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:42 INFO 140411496507200] Epoch[2] Batch [5]#011Speed: 510.68 samples/sec#011loss=-1.698780\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:43 INFO 140411496507200] processed a total of 1262 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5487.214803695679, \"sum\": 5487.214803695679, \"min\": 5487.214803695679}}, \"EndTime\": 1574322283.9847, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322278.497426}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:43 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=229.984257505 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:43 INFO 140411496507200] #progress_metric: host=algo-1, completed 6 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:43 INFO 140411496507200] #quality_metric: host=algo-1, epoch=2, train loss <loss>=-1.85381650925\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:43 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:44 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_759c6c9c-a264-43d5-98a5-3f2a9077e6b2-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.86894416809082, \"sum\": 21.86894416809082, \"min\": 21.86894416809082}}, \"EndTime\": 1574322284.007147, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322283.984781}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:47 INFO 140411496507200] Epoch[3] Batch[0] avg_epoch_loss=-1.780798\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:47 INFO 140411496507200] #quality_metric: host=algo-1, epoch=3, batch=0 train loss <loss>=-1.780798316\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:48 INFO 140411496507200] Epoch[3] Batch[5] avg_epoch_loss=-2.007487\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:48 INFO 140411496507200] #quality_metric: host=algo-1, epoch=3, batch=5 train loss <loss>=-2.00748664141\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:48 INFO 140411496507200] Epoch[3] Batch [5]#011Speed: 522.77 samples/sec#011loss=-2.007487\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:49 INFO 140411496507200] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5477.966070175171, \"sum\": 5477.966070175171, \"min\": 5477.966070175171}}, \"EndTime\": 1574322289.485246, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322284.007221}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:49 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=231.650967609 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:49 INFO 140411496507200] #progress_metric: host=algo-1, completed 8 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:49 INFO 140411496507200] #quality_metric: host=algo-1, epoch=3, train loss <loss>=-2.04101966619\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:49 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:49 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_db4a62ff-b652-4959-a959-8f3254f7aa86-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 15.01607894897461, \"sum\": 15.01607894897461, \"min\": 15.01607894897461}}, \"EndTime\": 1574322289.500856, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322289.485312}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:52 INFO 140411496507200] Epoch[4] Batch[0] avg_epoch_loss=-2.095732\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:52 INFO 140411496507200] #quality_metric: host=algo-1, epoch=4, batch=0 train loss <loss>=-2.09573173523\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:53 INFO 140411496507200] Epoch[4] Batch[5] avg_epoch_loss=-2.109630\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:53 INFO 140411496507200] #quality_metric: host=algo-1, epoch=4, batch=5 train loss <loss>=-2.10962971052\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:53 INFO 140411496507200] Epoch[4] Batch [5]#011Speed: 499.70 samples/sec#011loss=-2.109630\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:54 INFO 140411496507200] processed a total of 1219 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5462.612152099609, \"sum\": 5462.612152099609, \"min\": 5462.612152099609}}, \"EndTime\": 1574322294.963588, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322289.500924}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:54 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=223.149514718 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:54 INFO 140411496507200] #progress_metric: host=algo-1, completed 10 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:54 INFO 140411496507200] #quality_metric: host=algo-1, epoch=4, train loss <loss>=-2.18188261986\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:54 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:54 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_48779d0b-7ad3-41b3-ac4c-25db2b3f1e48-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 13.694047927856445, \"sum\": 13.694047927856445, \"min\": 13.694047927856445}}, \"EndTime\": 1574322294.977861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322294.963648}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:57 INFO 140411496507200] Epoch[5] Batch[0] avg_epoch_loss=-2.372119\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:57 INFO 140411496507200] #quality_metric: host=algo-1, epoch=5, batch=0 train loss <loss>=-2.37211871147\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:59 INFO 140411496507200] Epoch[5] Batch[5] avg_epoch_loss=-2.325673\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:59 INFO 140411496507200] #quality_metric: host=algo-1, epoch=5, batch=5 train loss <loss>=-2.32567288478\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:44:59 INFO 140411496507200] Epoch[5] Batch [5]#011Speed: 540.96 samples/sec#011loss=-2.325673\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:00 INFO 140411496507200] processed a total of 1271 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5392.247915267944, \"sum\": 5392.247915267944, \"min\": 5392.247915267944}}, \"EndTime\": 1574322300.370234, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322294.97793}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:00 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=235.703713411 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:00 INFO 140411496507200] #progress_metric: host=algo-1, completed 12 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:00 INFO 140411496507200] #quality_metric: host=algo-1, epoch=5, train loss <loss>=-2.35747822523\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:00 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:00 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_d76ab279-ff31-4818-a53e-a722a1c77df6-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.512985229492188, \"sum\": 21.512985229492188, \"min\": 21.512985229492188}}, \"EndTime\": 1574322300.392306, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322300.370314}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:03 INFO 140411496507200] Epoch[6] Batch[0] avg_epoch_loss=-2.346293\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:03 INFO 140411496507200] #quality_metric: host=algo-1, epoch=6, batch=0 train loss <loss>=-2.3462934494\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:04 INFO 140411496507200] Epoch[6] Batch[5] avg_epoch_loss=-2.460202\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:04 INFO 140411496507200] #quality_metric: host=algo-1, epoch=6, batch=5 train loss <loss>=-2.46020174026\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:04 INFO 140411496507200] Epoch[6] Batch [5]#011Speed: 508.18 samples/sec#011loss=-2.460202\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:05 INFO 140411496507200] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5467.450857162476, \"sum\": 5467.450857162476, \"min\": 5467.450857162476}}, \"EndTime\": 1574322305.859885, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322300.392367}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:05 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=229.901162178 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:05 INFO 140411496507200] #progress_metric: host=algo-1, completed 14 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:05 INFO 140411496507200] #quality_metric: host=algo-1, epoch=6, train loss <loss>=-2.62130725384\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:05 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:05 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_74325d81-5d62-4648-b2f0-23daf0e71a3b-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.159887313842773, \"sum\": 21.159887313842773, \"min\": 21.159887313842773}}, \"EndTime\": 1574322305.881626, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322305.859966}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:08 INFO 140411496507200] Epoch[7] Batch[0] avg_epoch_loss=-2.722556\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:08 INFO 140411496507200] #quality_metric: host=algo-1, epoch=7, batch=0 train loss <loss>=-2.7225561142\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:10 INFO 140411496507200] Epoch[7] Batch[5] avg_epoch_loss=-2.598397\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:10 INFO 140411496507200] #quality_metric: host=algo-1, epoch=7, batch=5 train loss <loss>=-2.59839653969\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:10 INFO 140411496507200] Epoch[7] Batch [5]#011Speed: 530.21 samples/sec#011loss=-2.598397\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] Epoch[7] Batch[10] avg_epoch_loss=-2.614299\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] #quality_metric: host=algo-1, epoch=7, batch=10 train loss <loss>=-2.63338165283\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] Epoch[7] Batch [10]#011Speed: 434.10 samples/sec#011loss=-2.633382\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5721.956968307495, \"sum\": 5721.956968307495, \"min\": 5721.956968307495}}, \"EndTime\": 1574322311.603697, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322305.881686}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=232.783096056 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] #progress_metric: host=algo-1, completed 16 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] #quality_metric: host=algo-1, epoch=7, train loss <loss>=-2.61429886384\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:11 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:14 INFO 140411496507200] Epoch[8] Batch[0] avg_epoch_loss=-2.871650\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:14 INFO 140411496507200] #quality_metric: host=algo-1, epoch=8, batch=0 train loss <loss>=-2.87164998055\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:15 INFO 140411496507200] Epoch[8] Batch[5] avg_epoch_loss=-2.602359\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:15 INFO 140411496507200] #quality_metric: host=algo-1, epoch=8, batch=5 train loss <loss>=-2.60235901674\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:15 INFO 140411496507200] Epoch[8] Batch [5]#011Speed: 527.97 samples/sec#011loss=-2.602359\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:16 INFO 140411496507200] processed a total of 1260 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5266.33882522583, \"sum\": 5266.33882522583, \"min\": 5266.33882522583}}, \"EndTime\": 1574322316.870553, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322311.60377}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:16 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=239.250807074 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:16 INFO 140411496507200] #progress_metric: host=algo-1, completed 18 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:16 INFO 140411496507200] #quality_metric: host=algo-1, epoch=8, train loss <loss>=-2.53425729275\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:16 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:19 INFO 140411496507200] Epoch[9] Batch[0] avg_epoch_loss=-2.723830\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:19 INFO 140411496507200] #quality_metric: host=algo-1, epoch=9, batch=0 train loss <loss>=-2.72382950783\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:21 INFO 140411496507200] Epoch[9] Batch[5] avg_epoch_loss=-2.726713\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:21 INFO 140411496507200] #quality_metric: host=algo-1, epoch=9, batch=5 train loss <loss>=-2.72671294212\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:21 INFO 140411496507200] Epoch[9] Batch [5]#011Speed: 516.76 samples/sec#011loss=-2.726713\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:22 INFO 140411496507200] processed a total of 1257 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5469.216108322144, \"sum\": 5469.216108322144, \"min\": 5469.216108322144}}, \"EndTime\": 1574322322.340318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322316.870624}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:22 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=229.82599878 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:22 INFO 140411496507200] #progress_metric: host=algo-1, completed 20 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:22 INFO 140411496507200] #quality_metric: host=algo-1, epoch=9, train loss <loss>=-2.67688586712\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:22 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:22 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_c416ac85-cde8-41d3-9f15-6f84a30634ac-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 14.50204849243164, \"sum\": 14.50204849243164, \"min\": 14.50204849243164}}, \"EndTime\": 1574322322.355443, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322322.34042}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:25 INFO 140411496507200] Epoch[10] Batch[0] avg_epoch_loss=-2.323921\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:25 INFO 140411496507200] #quality_metric: host=algo-1, epoch=10, batch=0 train loss <loss>=-2.32392096519\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:26 INFO 140411496507200] Epoch[10] Batch[5] avg_epoch_loss=-2.568121\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:26 INFO 140411496507200] #quality_metric: host=algo-1, epoch=10, batch=5 train loss <loss>=-2.56812131405\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:26 INFO 140411496507200] Epoch[10] Batch [5]#011Speed: 494.91 samples/sec#011loss=-2.568121\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] Epoch[10] Batch[10] avg_epoch_loss=-2.817473\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] #quality_metric: host=algo-1, epoch=10, batch=10 train loss <loss>=-3.11669402122\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] Epoch[10] Batch [10]#011Speed: 395.72 samples/sec#011loss=-3.116694\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] processed a total of 1332 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5913.611888885498, \"sum\": 5913.611888885498, \"min\": 5913.611888885498}}, \"EndTime\": 1574322328.269173, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322322.355505}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=225.239203335 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] #progress_metric: host=algo-1, completed 22 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] #quality_metric: host=algo-1, epoch=10, train loss <loss>=-2.81747254458\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:28 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_3c33afb6-4cd9-446f-8505-96191c935ded-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.464109420776367, \"sum\": 21.464109420776367, \"min\": 21.464109420776367}}, \"EndTime\": 1574322328.291157, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322328.269242}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:31 INFO 140411496507200] Epoch[11] Batch[0] avg_epoch_loss=-2.774922\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:31 INFO 140411496507200] #quality_metric: host=algo-1, epoch=11, batch=0 train loss <loss>=-2.77492213249\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:32 INFO 140411496507200] Epoch[11] Batch[5] avg_epoch_loss=-2.737410\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:32 INFO 140411496507200] #quality_metric: host=algo-1, epoch=11, batch=5 train loss <loss>=-2.73741022746\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:32 INFO 140411496507200] Epoch[11] Batch [5]#011Speed: 487.62 samples/sec#011loss=-2.737410\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] Epoch[11] Batch[10] avg_epoch_loss=-2.718845\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] #quality_metric: host=algo-1, epoch=11, batch=10 train loss <loss>=-2.69656772614\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] Epoch[11] Batch [10]#011Speed: 400.80 samples/sec#011loss=-2.696568\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] processed a total of 1290 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5919.417142868042, \"sum\": 5919.417142868042, \"min\": 5919.417142868042}}, \"EndTime\": 1574322334.210698, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322328.291227}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=217.92266595 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] #progress_metric: host=algo-1, completed 24 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] #quality_metric: host=algo-1, epoch=11, train loss <loss>=-2.71884545413\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:34 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:37 INFO 140411496507200] Epoch[12] Batch[0] avg_epoch_loss=-2.549273\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:37 INFO 140411496507200] #quality_metric: host=algo-1, epoch=12, batch=0 train loss <loss>=-2.54927325249\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:38 INFO 140411496507200] Epoch[12] Batch[5] avg_epoch_loss=-2.737784\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:38 INFO 140411496507200] #quality_metric: host=algo-1, epoch=12, batch=5 train loss <loss>=-2.73778378963\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:38 INFO 140411496507200] Epoch[12] Batch [5]#011Speed: 507.72 samples/sec#011loss=-2.737784\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] Epoch[12] Batch[10] avg_epoch_loss=-2.779085\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] #quality_metric: host=algo-1, epoch=12, batch=10 train loss <loss>=-2.82864723206\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] Epoch[12] Batch [10]#011Speed: 408.55 samples/sec#011loss=-2.828647\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5795.322895050049, \"sum\": 5795.322895050049, \"min\": 5795.322895050049}}, \"EndTime\": 1574322340.006557, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322334.210779}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=222.0712294 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] #progress_metric: host=algo-1, completed 26 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] #quality_metric: host=algo-1, epoch=12, train loss <loss>=-2.77908535437\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:40 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:43 INFO 140411496507200] Epoch[13] Batch[0] avg_epoch_loss=-2.803831\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:43 INFO 140411496507200] #quality_metric: host=algo-1, epoch=13, batch=0 train loss <loss>=-2.80383062363\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:44 INFO 140411496507200] Epoch[13] Batch[5] avg_epoch_loss=-2.745246\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:44 INFO 140411496507200] #quality_metric: host=algo-1, epoch=13, batch=5 train loss <loss>=-2.74524565538\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:44 INFO 140411496507200] Epoch[13] Batch [5]#011Speed: 519.01 samples/sec#011loss=-2.745246\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] Epoch[13] Batch[10] avg_epoch_loss=-2.811180\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] #quality_metric: host=algo-1, epoch=13, batch=10 train loss <loss>=-2.89030165672\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] Epoch[13] Batch [10]#011Speed: 427.82 samples/sec#011loss=-2.890302\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5766.772031784058, \"sum\": 5766.772031784058, \"min\": 5766.772031784058}}, \"EndTime\": 1574322345.773871, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322340.006636}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=228.546419199 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] #progress_metric: host=algo-1, completed 28 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] #quality_metric: host=algo-1, epoch=13, train loss <loss>=-2.81118020144\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:45 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:48 INFO 140411496507200] Epoch[14] Batch[0] avg_epoch_loss=-2.550368\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:48 INFO 140411496507200] #quality_metric: host=algo-1, epoch=14, batch=0 train loss <loss>=-2.55036759377\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:49 INFO 140411496507200] Epoch[14] Batch[5] avg_epoch_loss=-2.595710\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:49 INFO 140411496507200] #quality_metric: host=algo-1, epoch=14, batch=5 train loss <loss>=-2.59571011861\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:49 INFO 140411496507200] Epoch[14] Batch [5]#011Speed: 530.26 samples/sec#011loss=-2.595710\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] Epoch[14] Batch[10] avg_epoch_loss=-2.655714\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] #quality_metric: host=algo-1, epoch=14, batch=10 train loss <loss>=-2.72771792412\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] Epoch[14] Batch [10]#011Speed: 408.46 samples/sec#011loss=-2.727718\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] processed a total of 1287 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5705.610036849976, \"sum\": 5705.610036849976, \"min\": 5705.610036849976}}, \"EndTime\": 1574322351.480021, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322345.773944}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=225.563714978 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] #progress_metric: host=algo-1, completed 30 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] #quality_metric: host=algo-1, epoch=14, train loss <loss>=-2.65571366657\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:51 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:54 INFO 140411496507200] Epoch[15] Batch[0] avg_epoch_loss=-3.019946\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:54 INFO 140411496507200] #quality_metric: host=algo-1, epoch=15, batch=0 train loss <loss>=-3.01994585991\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:55 INFO 140411496507200] Epoch[15] Batch[5] avg_epoch_loss=-2.824105\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:55 INFO 140411496507200] #quality_metric: host=algo-1, epoch=15, batch=5 train loss <loss>=-2.82410506407\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:55 INFO 140411496507200] Epoch[15] Batch [5]#011Speed: 501.67 samples/sec#011loss=-2.824105\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] Epoch[15] Batch[10] avg_epoch_loss=-2.785096\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] #quality_metric: host=algo-1, epoch=15, batch=10 train loss <loss>=-2.73828594685\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] Epoch[15] Batch [10]#011Speed: 439.18 samples/sec#011loss=-2.738286\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] processed a total of 1345 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5762.35294342041, \"sum\": 5762.35294342041, \"min\": 5762.35294342041}}, \"EndTime\": 1574322357.242921, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322351.480083}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=233.407018935 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] #progress_metric: host=algo-1, completed 32 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] #quality_metric: host=algo-1, epoch=15, train loss <loss>=-2.78509637443\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:45:57 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:00 INFO 140411496507200] Epoch[16] Batch[0] avg_epoch_loss=-2.795537\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:00 INFO 140411496507200] #quality_metric: host=algo-1, epoch=16, batch=0 train loss <loss>=-2.79553747177\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:01 INFO 140411496507200] Epoch[16] Batch[5] avg_epoch_loss=-2.694205\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:01 INFO 140411496507200] #quality_metric: host=algo-1, epoch=16, batch=5 train loss <loss>=-2.69420484702\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:01 INFO 140411496507200] Epoch[16] Batch [5]#011Speed: 531.34 samples/sec#011loss=-2.694205\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:02 INFO 140411496507200] processed a total of 1221 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5260.509967803955, \"sum\": 5260.509967803955, \"min\": 5260.509967803955}}, \"EndTime\": 1574322362.503943, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322357.242995}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:02 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=232.102840067 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:02 INFO 140411496507200] #progress_metric: host=algo-1, completed 34 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:02 INFO 140411496507200] #quality_metric: host=algo-1, epoch=16, train loss <loss>=-2.70818309784\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:02 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:05 INFO 140411496507200] Epoch[17] Batch[0] avg_epoch_loss=-2.722202\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:05 INFO 140411496507200] #quality_metric: host=algo-1, epoch=17, batch=0 train loss <loss>=-2.72220206261\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:06 INFO 140411496507200] Epoch[17] Batch[5] avg_epoch_loss=-2.810900\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:06 INFO 140411496507200] #quality_metric: host=algo-1, epoch=17, batch=5 train loss <loss>=-2.81089993318\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:06 INFO 140411496507200] Epoch[17] Batch [5]#011Speed: 520.71 samples/sec#011loss=-2.810900\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] Epoch[17] Batch[10] avg_epoch_loss=-2.793382\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] #quality_metric: host=algo-1, epoch=17, batch=10 train loss <loss>=-2.77236065865\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] Epoch[17] Batch [10]#011Speed: 421.46 samples/sec#011loss=-2.772361\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] processed a total of 1334 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5783.8029861450195, \"sum\": 5783.8029861450195, \"min\": 5783.8029861450195}}, \"EndTime\": 1574322368.288293, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322362.504003}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=230.638151997 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] #progress_metric: host=algo-1, completed 36 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] #quality_metric: host=algo-1, epoch=17, train loss <loss>=-2.79338208112\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:08 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:11 INFO 140411496507200] Epoch[18] Batch[0] avg_epoch_loss=-2.785065\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:11 INFO 140411496507200] #quality_metric: host=algo-1, epoch=18, batch=0 train loss <loss>=-2.78506469727\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:12 INFO 140411496507200] Epoch[18] Batch[5] avg_epoch_loss=-2.934454\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:12 INFO 140411496507200] #quality_metric: host=algo-1, epoch=18, batch=5 train loss <loss>=-2.93445400397\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:12 INFO 140411496507200] Epoch[18] Batch [5]#011Speed: 524.52 samples/sec#011loss=-2.934454\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:13 INFO 140411496507200] processed a total of 1226 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5294.702053070068, \"sum\": 5294.702053070068, \"min\": 5294.702053070068}}, \"EndTime\": 1574322373.58358, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322368.288403}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:13 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=231.547587868 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:13 INFO 140411496507200] #progress_metric: host=algo-1, completed 38 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:13 INFO 140411496507200] #quality_metric: host=algo-1, epoch=18, train loss <loss>=-2.91025271416\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:13 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:13 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_688ebdc5-48e1-423e-8344-ac84fd6ed333-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 14.394998550415039, \"sum\": 14.394998550415039, \"min\": 14.394998550415039}}, \"EndTime\": 1574322373.598527, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322373.583648}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:16 INFO 140411496507200] Epoch[19] Batch[0] avg_epoch_loss=-2.907380\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:16 INFO 140411496507200] #quality_metric: host=algo-1, epoch=19, batch=0 train loss <loss>=-2.90738010406\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:17 INFO 140411496507200] Epoch[19] Batch[5] avg_epoch_loss=-2.883934\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:17 INFO 140411496507200] #quality_metric: host=algo-1, epoch=19, batch=5 train loss <loss>=-2.88393417994\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:17 INFO 140411496507200] Epoch[19] Batch [5]#011Speed: 521.24 samples/sec#011loss=-2.883934\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] Epoch[19] Batch[10] avg_epoch_loss=-2.617990\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] #quality_metric: host=algo-1, epoch=19, batch=10 train loss <loss>=-2.29885641038\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] Epoch[19] Batch [10]#011Speed: 398.63 samples/sec#011loss=-2.298856\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] processed a total of 1292 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5773.437023162842, \"sum\": 5773.437023162842, \"min\": 5773.437023162842}}, \"EndTime\": 1574322379.37208, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322373.598589}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=223.779286486 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] #progress_metric: host=algo-1, completed 40 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] #quality_metric: host=algo-1, epoch=19, train loss <loss>=-2.61798973923\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:19 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:22 INFO 140411496507200] Epoch[20] Batch[0] avg_epoch_loss=-2.537968\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:22 INFO 140411496507200] #quality_metric: host=algo-1, epoch=20, batch=0 train loss <loss>=-2.53796839714\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:23 INFO 140411496507200] Epoch[20] Batch[5] avg_epoch_loss=-2.901859\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:23 INFO 140411496507200] #quality_metric: host=algo-1, epoch=20, batch=5 train loss <loss>=-2.90185896556\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:23 INFO 140411496507200] Epoch[20] Batch [5]#011Speed: 526.85 samples/sec#011loss=-2.901859\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] Epoch[20] Batch[10] avg_epoch_loss=-2.970478\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] #quality_metric: host=algo-1, epoch=20, batch=10 train loss <loss>=-3.05281991959\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] Epoch[20] Batch [10]#011Speed: 416.55 samples/sec#011loss=-3.052820\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] processed a total of 1291 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5750.0, \"sum\": 5750.0, \"min\": 5750.0}}, \"EndTime\": 1574322385.122602, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322379.372155}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=224.517559201 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] #progress_metric: host=algo-1, completed 42 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] #quality_metric: host=algo-1, epoch=20, train loss <loss>=-2.97047758102\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:25 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_b321ef47-60ea-4c55-af50-1a187d4c80ef-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 14.760017395019531, \"sum\": 14.760017395019531, \"min\": 14.760017395019531}}, \"EndTime\": 1574322385.137944, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322385.122673}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:28 INFO 140411496507200] Epoch[21] Batch[0] avg_epoch_loss=-2.648188\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:28 INFO 140411496507200] #quality_metric: host=algo-1, epoch=21, batch=0 train loss <loss>=-2.64818811417\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:29 INFO 140411496507200] Epoch[21] Batch[5] avg_epoch_loss=-2.783399\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:29 INFO 140411496507200] #quality_metric: host=algo-1, epoch=21, batch=5 train loss <loss>=-2.78339854876\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:29 INFO 140411496507200] Epoch[21] Batch [5]#011Speed: 487.41 samples/sec#011loss=-2.783399\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:30 INFO 140411496507200] processed a total of 1264 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5457.705020904541, \"sum\": 5457.705020904541, \"min\": 5457.705020904541}}, \"EndTime\": 1574322390.595786, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322385.138022}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:30 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=231.594261691 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:30 INFO 140411496507200] #progress_metric: host=algo-1, completed 44 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:30 INFO 140411496507200] #quality_metric: host=algo-1, epoch=21, train loss <loss>=-2.77944989204\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:30 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:33 INFO 140411496507200] Epoch[22] Batch[0] avg_epoch_loss=-3.214964\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:33 INFO 140411496507200] #quality_metric: host=algo-1, epoch=22, batch=0 train loss <loss>=-3.21496391296\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:34 INFO 140411496507200] Epoch[22] Batch[5] avg_epoch_loss=-3.021647\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:34 INFO 140411496507200] #quality_metric: host=algo-1, epoch=22, batch=5 train loss <loss>=-3.02164661884\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:34 INFO 140411496507200] Epoch[22] Batch [5]#011Speed: 513.80 samples/sec#011loss=-3.021647\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] Epoch[22] Batch[10] avg_epoch_loss=-3.060359\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] #quality_metric: host=algo-1, epoch=22, batch=10 train loss <loss>=-3.10681447983\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] Epoch[22] Batch [10]#011Speed: 388.85 samples/sec#011loss=-3.106814\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] processed a total of 1288 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5838.016033172607, \"sum\": 5838.016033172607, \"min\": 5838.016033172607}}, \"EndTime\": 1574322396.434317, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322390.595866}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=220.61858859 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] #progress_metric: host=algo-1, completed 46 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] #quality_metric: host=algo-1, epoch=22, train loss <loss>=-3.06035928293\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:36 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_523c58d8-f0e0-43e2-9f40-b7781a9a8d26-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.368026733398438, \"sum\": 21.368026733398438, \"min\": 21.368026733398438}}, \"EndTime\": 1574322396.456233, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322396.434394}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:39 INFO 140411496507200] Epoch[23] Batch[0] avg_epoch_loss=-2.998892\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:39 INFO 140411496507200] #quality_metric: host=algo-1, epoch=23, batch=0 train loss <loss>=-2.99889230728\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:40 INFO 140411496507200] Epoch[23] Batch[5] avg_epoch_loss=-2.768222\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:40 INFO 140411496507200] #quality_metric: host=algo-1, epoch=23, batch=5 train loss <loss>=-2.76822177569\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:40 INFO 140411496507200] Epoch[23] Batch [5]#011Speed: 478.73 samples/sec#011loss=-2.768222\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:41 INFO 140411496507200] processed a total of 1269 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5505.4638385772705, \"sum\": 5505.4638385772705, \"min\": 5505.4638385772705}}, \"EndTime\": 1574322401.961824, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322396.456302}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:41 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=230.493299069 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:41 INFO 140411496507200] #progress_metric: host=algo-1, completed 48 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:41 INFO 140411496507200] #quality_metric: host=algo-1, epoch=23, train loss <loss>=-2.8587379694\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:41 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:44 INFO 140411496507200] Epoch[24] Batch[0] avg_epoch_loss=-2.727322\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:44 INFO 140411496507200] #quality_metric: host=algo-1, epoch=24, batch=0 train loss <loss>=-2.72732162476\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:46 INFO 140411496507200] Epoch[24] Batch[5] avg_epoch_loss=-2.952083\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:46 INFO 140411496507200] #quality_metric: host=algo-1, epoch=24, batch=5 train loss <loss>=-2.95208303134\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:46 INFO 140411496507200] Epoch[24] Batch [5]#011Speed: 517.68 samples/sec#011loss=-2.952083\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] Epoch[24] Batch[10] avg_epoch_loss=-2.870559\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] #quality_metric: host=algo-1, epoch=24, batch=10 train loss <loss>=-2.77273044586\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] Epoch[24] Batch [10]#011Speed: 405.66 samples/sec#011loss=-2.772730\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] processed a total of 1281 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5744.836091995239, \"sum\": 5744.836091995239, \"min\": 5744.836091995239}}, \"EndTime\": 1574322407.7072, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322401.961907}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=222.978791402 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] #progress_metric: host=algo-1, completed 50 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] #quality_metric: host=algo-1, epoch=24, train loss <loss>=-2.87055912885\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:47 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:50 INFO 140411496507200] Epoch[25] Batch[0] avg_epoch_loss=-2.768388\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:50 INFO 140411496507200] #quality_metric: host=algo-1, epoch=25, batch=0 train loss <loss>=-2.76838803291\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:51 INFO 140411496507200] Epoch[25] Batch[5] avg_epoch_loss=-2.909126\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:51 INFO 140411496507200] #quality_metric: host=algo-1, epoch=25, batch=5 train loss <loss>=-2.90912592411\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:51 INFO 140411496507200] Epoch[25] Batch [5]#011Speed: 520.73 samples/sec#011loss=-2.909126\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] Epoch[25] Batch[10] avg_epoch_loss=-3.178828\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] #quality_metric: host=algo-1, epoch=25, batch=10 train loss <loss>=-3.50247039795\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] Epoch[25] Batch [10]#011Speed: 413.10 samples/sec#011loss=-3.502470\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] processed a total of 1310 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5807.593107223511, \"sum\": 5807.593107223511, \"min\": 5807.593107223511}}, \"EndTime\": 1574322413.515332, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322407.707273}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=225.562303471 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] #progress_metric: host=algo-1, completed 52 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] #quality_metric: host=algo-1, epoch=25, train loss <loss>=-3.17882795767\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] best epoch loss so far\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:53 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/state_4e21af9b-abb3-4122-92ae-b774a966e983-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.serialize.time\": {\"count\": 1, \"max\": 21.595001220703125, \"sum\": 21.595001220703125, \"min\": 21.595001220703125}}, \"EndTime\": 1574322413.537498, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322413.51541}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:56 INFO 140411496507200] Epoch[26] Batch[0] avg_epoch_loss=-2.875034\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:56 INFO 140411496507200] #quality_metric: host=algo-1, epoch=26, batch=0 train loss <loss>=-2.87503385544\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:57 INFO 140411496507200] Epoch[26] Batch[5] avg_epoch_loss=-2.861421\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:57 INFO 140411496507200] #quality_metric: host=algo-1, epoch=26, batch=5 train loss <loss>=-2.86142079035\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:57 INFO 140411496507200] Epoch[26] Batch [5]#011Speed: 525.07 samples/sec#011loss=-2.861421\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:58 INFO 140411496507200] processed a total of 1236 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5316.680908203125, \"sum\": 5316.680908203125, \"min\": 5316.680908203125}}, \"EndTime\": 1574322418.854318, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322413.537576}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:58 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=232.471268957 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:58 INFO 140411496507200] #progress_metric: host=algo-1, completed 54 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:58 INFO 140411496507200] #quality_metric: host=algo-1, epoch=26, train loss <loss>=-2.7792006731\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:46:58 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:01 INFO 140411496507200] Epoch[27] Batch[0] avg_epoch_loss=-2.679491\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:01 INFO 140411496507200] #quality_metric: host=algo-1, epoch=27, batch=0 train loss <loss>=-2.67949080467\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:02 INFO 140411496507200] Epoch[27] Batch[5] avg_epoch_loss=-2.577116\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:02 INFO 140411496507200] #quality_metric: host=algo-1, epoch=27, batch=5 train loss <loss>=-2.57711581389\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:02 INFO 140411496507200] Epoch[27] Batch [5]#011Speed: 496.18 samples/sec#011loss=-2.577116\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:04 INFO 140411496507200] processed a total of 1244 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5302.622079849243, \"sum\": 5302.622079849243, \"min\": 5302.622079849243}}, \"EndTime\": 1574322424.157469, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322418.854388}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:04 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=234.59577967 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:04 INFO 140411496507200] #progress_metric: host=algo-1, completed 56 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:04 INFO 140411496507200] #quality_metric: host=algo-1, epoch=27, train loss <loss>=-2.61782078743\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:04 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:07 INFO 140411496507200] Epoch[28] Batch[0] avg_epoch_loss=-3.285250\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:07 INFO 140411496507200] #quality_metric: host=algo-1, epoch=28, batch=0 train loss <loss>=-3.28525018692\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:08 INFO 140411496507200] Epoch[28] Batch[5] avg_epoch_loss=-2.950327\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:08 INFO 140411496507200] #quality_metric: host=algo-1, epoch=28, batch=5 train loss <loss>=-2.95032695929\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:08 INFO 140411496507200] Epoch[28] Batch [5]#011Speed: 481.84 samples/sec#011loss=-2.950327\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] Epoch[28] Batch[10] avg_epoch_loss=-2.658237\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] #quality_metric: host=algo-1, epoch=28, batch=10 train loss <loss>=-2.30772923231\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] Epoch[28] Batch [10]#011Speed: 419.13 samples/sec#011loss=-2.307729\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] processed a total of 1304 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5856.068849563599, \"sum\": 5856.068849563599, \"min\": 5856.068849563599}}, \"EndTime\": 1574322430.014099, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322424.157549}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=222.670372294 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] #progress_metric: host=algo-1, completed 58 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] #quality_metric: host=algo-1, epoch=28, train loss <loss>=-2.65823708339\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:10 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:13 INFO 140411496507200] Epoch[29] Batch[0] avg_epoch_loss=-2.594349\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:13 INFO 140411496507200] #quality_metric: host=algo-1, epoch=29, batch=0 train loss <loss>=-2.59434890747\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:14 INFO 140411496507200] Epoch[29] Batch[5] avg_epoch_loss=-2.857423\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:14 INFO 140411496507200] #quality_metric: host=algo-1, epoch=29, batch=5 train loss <loss>=-2.85742262999\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:14 INFO 140411496507200] Epoch[29] Batch [5]#011Speed: 523.53 samples/sec#011loss=-2.857423\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] Epoch[29] Batch[10] avg_epoch_loss=-3.159467\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] #quality_metric: host=algo-1, epoch=29, batch=10 train loss <loss>=-3.52192072868\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] Epoch[29] Batch [10]#011Speed: 399.27 samples/sec#011loss=-3.521921\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] processed a total of 1284 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5825.522899627686, \"sum\": 5825.522899627686, \"min\": 5825.522899627686}}, \"EndTime\": 1574322435.840169, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322430.014182}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=220.405231362 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] #progress_metric: host=algo-1, completed 60 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] #quality_metric: host=algo-1, epoch=29, train loss <loss>=-3.15946722031\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:15 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:18 INFO 140411496507200] Epoch[30] Batch[0] avg_epoch_loss=-2.946551\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:18 INFO 140411496507200] #quality_metric: host=algo-1, epoch=30, batch=0 train loss <loss>=-2.94655132294\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:19 INFO 140411496507200] Epoch[30] Batch[5] avg_epoch_loss=-2.872061\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:19 INFO 140411496507200] #quality_metric: host=algo-1, epoch=30, batch=5 train loss <loss>=-2.87206145128\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:19 INFO 140411496507200] Epoch[30] Batch [5]#011Speed: 519.75 samples/sec#011loss=-2.872061\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:21 INFO 140411496507200] processed a total of 1229 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5372.140169143677, \"sum\": 5372.140169143677, \"min\": 5372.140169143677}}, \"EndTime\": 1574322441.212861, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322435.840241}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:21 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=228.7682848 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:21 INFO 140411496507200] #progress_metric: host=algo-1, completed 62 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:21 INFO 140411496507200] #quality_metric: host=algo-1, epoch=30, train loss <loss>=-2.97785532475\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:21 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:24 INFO 140411496507200] Epoch[31] Batch[0] avg_epoch_loss=-3.081515\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:24 INFO 140411496507200] #quality_metric: host=algo-1, epoch=31, batch=0 train loss <loss>=-3.08151459694\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:25 INFO 140411496507200] Epoch[31] Batch[5] avg_epoch_loss=-3.033165\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:25 INFO 140411496507200] #quality_metric: host=algo-1, epoch=31, batch=5 train loss <loss>=-3.03316521645\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:25 INFO 140411496507200] Epoch[31] Batch [5]#011Speed: 504.05 samples/sec#011loss=-3.033165\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] Epoch[31] Batch[10] avg_epoch_loss=-3.096449\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] #quality_metric: host=algo-1, epoch=31, batch=10 train loss <loss>=-3.17238888741\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] Epoch[31] Batch [10]#011Speed: 412.04 samples/sec#011loss=-3.172389\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] processed a total of 1318 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5863.676071166992, \"sum\": 5863.676071166992, \"min\": 5863.676071166992}}, \"EndTime\": 1574322447.07711, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322441.21293}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=224.769230031 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] #progress_metric: host=algo-1, completed 64 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] #quality_metric: host=algo-1, epoch=31, train loss <loss>=-3.09644870325\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:27 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:30 INFO 140411496507200] Epoch[32] Batch[0] avg_epoch_loss=-2.225262\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:30 INFO 140411496507200] #quality_metric: host=algo-1, epoch=32, batch=0 train loss <loss>=-2.22526168823\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:31 INFO 140411496507200] Epoch[32] Batch[5] avg_epoch_loss=-2.564181\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:31 INFO 140411496507200] #quality_metric: host=algo-1, epoch=32, batch=5 train loss <loss>=-2.56418124835\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:31 INFO 140411496507200] Epoch[32] Batch [5]#011Speed: 494.63 samples/sec#011loss=-2.564181\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:32 INFO 140411496507200] processed a total of 1255 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5510.69188117981, \"sum\": 5510.69188117981, \"min\": 5510.69188117981}}, \"EndTime\": 1574322452.588311, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322447.077189}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:32 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=227.73384673 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:32 INFO 140411496507200] #progress_metric: host=algo-1, completed 66 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:32 INFO 140411496507200] #quality_metric: host=algo-1, epoch=32, train loss <loss>=-2.72203018665\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:32 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:35 INFO 140411496507200] Epoch[33] Batch[0] avg_epoch_loss=-3.023395\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:35 INFO 140411496507200] #quality_metric: host=algo-1, epoch=33, batch=0 train loss <loss>=-3.02339506149\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:36 INFO 140411496507200] Epoch[33] Batch[5] avg_epoch_loss=-2.960325\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:36 INFO 140411496507200] #quality_metric: host=algo-1, epoch=33, batch=5 train loss <loss>=-2.96032472452\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:36 INFO 140411496507200] Epoch[33] Batch [5]#011Speed: 481.69 samples/sec#011loss=-2.960325\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] Epoch[33] Batch[10] avg_epoch_loss=-2.787302\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] #quality_metric: host=algo-1, epoch=33, batch=10 train loss <loss>=-2.57967550755\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] Epoch[33] Batch [10]#011Speed: 401.94 samples/sec#011loss=-2.579676\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] processed a total of 1289 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5847.733974456787, \"sum\": 5847.733974456787, \"min\": 5847.733974456787}}, \"EndTime\": 1574322458.436625, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322452.588407}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=220.42345345 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] #progress_metric: host=algo-1, completed 68 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] #quality_metric: host=algo-1, epoch=33, train loss <loss>=-2.78730235317\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:38 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\n",
      "2019-11-21 07:47:53 Uploading - Uploading generated training model\u001b[31m[11/21/2019 07:47:41 INFO 140411496507200] Epoch[34] Batch[0] avg_epoch_loss=-2.738097\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:41 INFO 140411496507200] #quality_metric: host=algo-1, epoch=34, batch=0 train loss <loss>=-2.73809742928\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:42 INFO 140411496507200] Epoch[34] Batch[5] avg_epoch_loss=-2.838377\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:42 INFO 140411496507200] #quality_metric: host=algo-1, epoch=34, batch=5 train loss <loss>=-2.83837707837\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:42 INFO 140411496507200] Epoch[34] Batch [5]#011Speed: 525.54 samples/sec#011loss=-2.838377\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:43 INFO 140411496507200] processed a total of 1232 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5298.568964004517, \"sum\": 5298.568964004517, \"min\": 5298.568964004517}}, \"EndTime\": 1574322463.735717, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322458.436691}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:43 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=232.510390335 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:43 INFO 140411496507200] #progress_metric: host=algo-1, completed 70 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:43 INFO 140411496507200] #quality_metric: host=algo-1, epoch=34, train loss <loss>=-2.94062361717\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:43 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:46 INFO 140411496507200] Epoch[35] Batch[0] avg_epoch_loss=-3.053041\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:46 INFO 140411496507200] #quality_metric: host=algo-1, epoch=35, batch=0 train loss <loss>=-3.05304098129\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:47 INFO 140411496507200] Epoch[35] Batch[5] avg_epoch_loss=-3.019408\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:47 INFO 140411496507200] #quality_metric: host=algo-1, epoch=35, batch=5 train loss <loss>=-3.01940818628\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:47 INFO 140411496507200] Epoch[35] Batch [5]#011Speed: 522.92 samples/sec#011loss=-3.019408\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] Epoch[35] Batch[10] avg_epoch_loss=-2.929574\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] #quality_metric: host=algo-1, epoch=35, batch=10 train loss <loss>=-2.82177228928\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] Epoch[35] Batch [10]#011Speed: 422.11 samples/sec#011loss=-2.821772\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] processed a total of 1307 examples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"update.time\": {\"count\": 1, \"max\": 5773.531913757324, \"sum\": 5773.531913757324, \"min\": 5773.531913757324}}, \"EndTime\": 1574322469.509792, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322463.7358}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] #throughput_metric: host=algo-1, train throughput=226.373454411 records/second\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] #progress_metric: host=algo-1, completed 72 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] #quality_metric: host=algo-1, epoch=35, train loss <loss>=-2.92957368764\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] loss did not improve\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] Loading parameters from best epoch (25)\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"state.deserialize.time\": {\"count\": 1, \"max\": 6.515979766845703, \"sum\": 6.515979766845703, \"min\": 6.515979766845703}}, \"EndTime\": 1574322469.51688, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322469.509869}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] stopping training now\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] #progress_metric: host=algo-1, completed 100 % of epochs\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] Final loss: -3.17882795767 (occurred at epoch 25)\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] #quality_metric: host=algo-1, train final_loss <loss>=-3.17882795767\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] Worker algo-1 finished training.\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 WARNING 140411496507200] wait_for_all_workers will not sync workers since the kv store is not running distributed\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:49 INFO 140411496507200] All workers finished. Serializing model for prediction.\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"get_graph.time\": {\"count\": 1, \"max\": 1496.169090270996, \"sum\": 1496.169090270996, \"min\": 1496.169090270996}}, \"EndTime\": 1574322471.01355, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322469.516929}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:51 INFO 140411496507200] Number of GPUs being used: 0\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"finalize.time\": {\"count\": 1, \"max\": 1657.2930812835693, \"sum\": 1657.2930812835693, \"min\": 1657.2930812835693}}, \"EndTime\": 1574322471.174636, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322471.013656}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:51 INFO 140411496507200] Serializing to /opt/ml/model/model_algo-1\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:51 INFO 140411496507200] Saved checkpoint to \"/opt/ml/model/model_algo-1-0000.params\"\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.serialize.time\": {\"count\": 1, \"max\": 6.355047225952148, \"sum\": 6.355047225952148, \"min\": 6.355047225952148}}, \"EndTime\": 1574322471.181082, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322471.174691}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:51 INFO 140411496507200] Successfully serialized the model for prediction.\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:51 INFO 140411496507200] Evaluating model accuracy on testset using 100 samples\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.bind.time\": {\"count\": 1, \"max\": 0.025033950805664062, \"sum\": 0.025033950805664062, \"min\": 0.025033950805664062}}, \"EndTime\": 1574322471.181836, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322471.181126}\n",
      "\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"model.score.time\": {\"count\": 1, \"max\": 1366.995096206665, \"sum\": 1366.995096206665, \"min\": 1366.995096206665}}, \"EndTime\": 1574322472.548819, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322471.181877}\n",
      "\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, RMSE): 0.00648011669954\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, mean_wQuantileLoss): 0.18828543\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.1]): 0.06728873\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.2]): 0.11587965\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.3]): 0.15930676\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.4]): 0.19231834\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.5]): 0.22079316\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.6]): 0.24066567\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.7]): 0.25190228\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.8]): 0.25040716\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #test_score (algo-1, wQuantileLoss[0.9]): 0.19600712\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #quality_metric: host=algo-1, test mean_wQuantileLoss <loss>=0.188285425305\u001b[0m\n",
      "\u001b[31m[11/21/2019 07:47:52 INFO 140411496507200] #quality_metric: host=algo-1, test RMSE <loss>=0.00648011669954\u001b[0m\n",
      "\u001b[31m#metrics {\"Metrics\": {\"totaltime\": {\"count\": 1, \"max\": 213807.0149421692, \"sum\": 213807.0149421692, \"min\": 213807.0149421692}, \"setuptime\": {\"count\": 1, \"max\": 8.703947067260742, \"sum\": 8.703947067260742, \"min\": 8.703947067260742}}, \"EndTime\": 1574322472.644908, \"Dimensions\": {\"Host\": \"algo-1\", \"Operation\": \"training\", \"Algorithm\": \"AWS/DeepAR\"}, \"StartTime\": 1574322472.548903}\n",
      "\u001b[0m\n",
      "\n",
      "2019-11-21 07:48:00 Completed - Training job completed\n",
      "Training seconds: 264\n",
      "Billable seconds: 264\n",
      "CPU times: user 973 ms, sys: 29.7 ms, total: 1 s\n",
      "Wall time: 7min 15s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# train and test channels\n",
    "data_channels = {\n",
    "    \"train\": train_s3_path,\n",
    "    \"test\": test_s3_path\n",
    "}\n",
    "\n",
    "# fit the estimator\n",
    "estimator.fit(inputs=data_channels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deploy and Create a Predictor\n",
    "\n",
    "Now that we have trained a model, we can use it to perform predictions by deploying it to a predictor endpoint.\n",
    "\n",
    "Remember to **delete the endpoint** at the end of this notebook. A cell at the very bottom of this notebook will be provided, but it is always good to keep, front-of-mind."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-----------------------------------------------------------------------------------------"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "# create a predictor\n",
    "predictor = estimator.deploy(\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.t2.medium',\n",
    "    content_type=\"application/json\" # specify that it will accept/produce JSON\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create endpoint from job name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------------------------------------------------------------!"
     ]
    }
   ],
   "source": [
    "job_name = \"forecasting-deepar-2019-11-21-07-41-20-349\"\n",
    "\n",
    "endpoint_name = sagemaker_session.endpoint_from_job(\n",
    "    job_name=job_name,\n",
    "    initial_instance_count=1,\n",
    "    instance_type='ml.m4.xlarge',\n",
    "    deployment_image=image_name,\n",
    "    role=role\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def json_predictor_input(input_ts, target_col, num_samples=50, quantiles=['0.1', '0.5', '0.9']):\n",
    "    '''Accepts a list of input time series and produces a formatted input.\n",
    "       :input_ts: An list of input time series.\n",
    "       :num_samples: Number of samples to calculate metrics with.\n",
    "       :quantiles: A list of quantiles to return in the predicted output.\n",
    "       :return: The JSON-formatted input.\n",
    "       '''\n",
    "    # request data is made of JSON objects (instances)\n",
    "    # and an output configuration that details the type of data/quantiles we want\n",
    "    \n",
    "    instances = []\n",
    "    \n",
    "    instances.append(series_to_json(input_ts, target_col))\n",
    "\n",
    "    # specify the output quantiles and samples\n",
    "    configuration = {\"num_samples\": num_samples, \n",
    "                     \"output_types\": [\"quantiles\"], \n",
    "                     \"quantiles\": quantiles}\n",
    "\n",
    "    request_data = {\"instances\": instances, \n",
    "                    \"configuration\": configuration}\n",
    "\n",
    "    json_request = json.dumps(request_data).encode('utf-8')\n",
    "    \n",
    "    return json_request"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Timedelta('0 days 01:00:00')"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_all_ts_data.index[1] - test_all_ts_data.index[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'freq' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9dee96cb9d4a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfreq\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'freq' is not defined"
     ]
    }
   ],
   "source": [
    "freq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictor = DeepARPredictor(\n",
    "    endpoint=endpoint_name,\n",
    "    sagemaker_session=sagemaker_session,\n",
    "    content_type=\"application/json\"\n",
    ")\n",
    "\n",
    "target_col = \"CabFlowMean\"\n",
    "predictor.set_prediction_parameters(freq, prediction_length, target_col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'builtin_function_or_method' object is not subscriptable",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-43-6059a9b3a7b5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mjson_prediction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredictor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_all_ts_data\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-39-8cbf8bc2dad0>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, ts, cat, encoding, num_samples, quantiles)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mprediction_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-39-8cbf8bc2dad0>\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m         \u001b[0mReturn\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mlist\u001b[0m \u001b[0mof\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mpandas\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0mobjects\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0meach\u001b[0m \u001b[0mcontaining\u001b[0m \u001b[0mthe\u001b[0m \u001b[0mpredictions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m         \"\"\"\n\u001b[0;32m---> 30\u001b[0;31m         \u001b[0mprediction_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m         \u001b[0mreq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__encode_request\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_samples\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mquantiles\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mres\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mDeepARPredictor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreq\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: 'builtin_function_or_method' object is not subscriptable"
     ]
    }
   ],
   "source": [
    "json_prediction = predictor.predict(test_all_ts_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# get all input and target (test) time series\n",
    "input_ts = test_all_ts_data\n",
    "\n",
    "# get formatted input time series\n",
    "json_input_ts = json_predictor_input(input_ts, target_col)\n",
    "\n",
    "# get the prediction from the predictor\n",
    "json_prediction = predictor.predict(json_input_ts)\n",
    "\n",
    "#print(json_prediction)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_amazonei_mxnet_p36",
   "language": "python",
   "name": "conda_amazonei_mxnet_p36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
